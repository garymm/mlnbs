{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601432967127",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering\n",
    "\n",
    "An attempt to match fastai's performance on [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/), but with pure pytorch, not using fastai's library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import fastai\n",
    "import fastai.collab\n",
    "import fastai.datasets\n",
    "import fastai.tabular.transform\n",
    "import math\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import typing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cpu\") # Seems to be much faster than GPU for this application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_path = fastai.datasets.download_data(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", ext=\"\")\n",
    "dest_dir = zip_path.parent\n",
    "data_dir = os.path.splitext(zip_path)[0]\n",
    "if not os.path.exists(data_dir):\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "n_item: 1682, n_user: 943\n"
    }
   ],
   "source": [
    "col_names = (\"user\", \"item\", \"rating\", \"timestamp\")\n",
    "train_df = pandas.read_csv(os.path.join(data_dir, \"ua.base\"), sep=\"\\t\", names=col_names)\n",
    "test_df = pandas.read_csv(os.path.join(data_dir, \"ua.test\"), sep=\"\\t\", names=col_names)\n",
    "concat_df = pandas.concat((train_df, test_df))\n",
    "n_item = concat_df[\"item\"].max()\n",
    "n_user = concat_df[\"user\"].max()\n",
    "print(f\"n_item: {n_item}, n_user: {n_user}\")\n",
    "\n",
    "class MovieLensDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pandas.DataFrame, device: torch.device):\n",
    "        # Indices into embeddings need to have dtype \"long\".\n",
    "        self.ids_tensor = torch.tensor(df[[\"user\", \"item\"]].to_numpy(), dtype=torch.long, device=device)\n",
    "        self.ratings_tensor = torch.tensor(df[[\"rating\"]].to_numpy(), dtype=torch.float, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids_tensor[idx], self.ratings_tensor[idx]\n",
    "        \n",
    "\n",
    "train_dataset = MovieLensDataset(train_df, dev)\n",
    "test_dataset = MovieLensDataset(test_df, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_inputs, test_labels = test_dataset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai benchmark\n",
    "fastai claims state of the art performance, so let's start with that and see how it does.\n",
    "This is based on <https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson4-collab.ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>user</th>\n      <th>item</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>778</td>\n      <td>69</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>815</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <td>632</td>\n      <td>1183</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>25</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>15</td>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Hacked up copy of CollabDataBunch.from_df because I want to use test_df as the validation set.\n",
    "user_name   = concat_df.columns[0]\n",
    "item_name   = concat_df.columns[1]\n",
    "rating_name = concat_df.columns[2]\n",
    "cat_names = [user_name,item_name]\n",
    "num_train = len(train_df)\n",
    "src = (fastai.collab.CollabList.from_df(concat_df, cat_names=cat_names, procs=fastai.tabular.transform.Categorify)\n",
    "        .split_by_idxs(train_idx=numpy.arange(num_train), valid_idx=numpy.arange(num_train, num_train + len(test_df)))\n",
    "        .label_from_df(cols=rating_name))\n",
    "data_bunch = src.databunch(path=\".\", bs=batch_size, val_bs=batch_size, device=dev)\n",
    "assert len(data_bunch.dl(fastai.basic_data.DatasetType.Train).dl.dataset.x) == num_train\n",
    "data_bunch.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.956673</td>\n      <td>1.053949</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.913251</td>\n      <td>0.989121</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.896868</td>\n      <td>0.958328</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.834530</td>\n      <td>0.954619</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.776227</td>\n      <td>0.926298</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.714198</td>\n      <td>0.914857</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.622492</td>\n      <td>0.903829</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.518370</td>\n      <td>0.899892</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.412492</td>\n      <td>0.898850</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.351790</td>\n      <td>0.899691</td>\n      <td>00:07</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "self.opt.lr = 0.003, self.opt.mom = 0.925\nself.opt.lr = 0.008, self.opt.mom = 0.875\nself.opt.lr = 0.010, self.opt.mom = 0.850\nself.opt.lr = 0.010, self.opt.mom = 0.855\nself.opt.lr = 0.008, self.opt.mom = 0.869\nself.opt.lr = 0.006, self.opt.mom = 0.889\nself.opt.lr = 0.004, self.opt.mom = 0.911\nself.opt.lr = 0.002, self.opt.mom = 0.931\nself.opt.lr = 0.000, self.opt.mom = 0.945\nself.opt.lr = 0.000, self.opt.mom = 0.950\n"
    }
   ],
   "source": [
    "fastai_learn = fastai.collab.collab_learner(data_bunch, n_factors=40, y_range=[0,5.5], wd=1e-1)\n",
    "fastai_learn.fit_one_cycle(num_epochs, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fastai_pred = fastai_learn.get_preds(ds_type=fastai.data_block.DatasetType.Valid)\n",
    "torch.nn.functional.mse_loss(*fastai_pred).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    def __init__(self, model: nn.Module, loss_func: nn.Module, optim: torch.optim.Optimizer):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.optim = optim\n",
    "\n",
    "    def fit(self, num_epochs: int):\n",
    "        print(\"epoch | train_loss | test_loss | time\")\n",
    "        for epoch in range(num_epochs):\n",
    "            start = time.time()\n",
    "            train_loss = torch.tensor([0.0], dtype=float, device=dev)\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader, 0):\n",
    "                train_loss += self._one_batch(inputs, targets)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(test_inputs[:, 0], test_inputs[:, 1])\n",
    "                test_loss = self.loss_func(pred, test_labels).item()\n",
    "\n",
    "            num_batches = batch_idx + 1\n",
    "            print(\"%5d |      %.3f |     %.3f |   %ds |\" % (\n",
    "                epoch,\n",
    "                train_loss / num_batches,\n",
    "                test_loss,\n",
    "                int(time.time() - start)))\n",
    "\n",
    "    def _one_batch(self, inputs: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "        self.optim.zero_grad()\n",
    "        print(inputs)\n",
    "        print(inputs.shape)\n",
    "        try:\n",
    "            pred = self.model(inputs[:, 0], inputs[:, 1])\n",
    "        except IndexError:\n",
    "            print(\"inputs.shape: %s\" % inputs.shape)\n",
    "            print(\"inputs: %s\" % inputs)\n",
    "            raise\n",
    "        loss = self.loss_func(pred, targets)\n",
    "        return loss\n",
    "\n",
    "class FitterOneCycle(Fitter):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.scheduler = None\n",
    "    \n",
    "    def fit(self, num_epochs: int):\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optim, max_lr=self.optim.defaults[\"lr\"], epochs=num_epochs,\n",
    "            steps_per_epoch=math.ceil(len(train_df) / batch_size))\n",
    "        super().fit(num_epochs)\n",
    "\n",
    "    def _one_batch(self, inputs: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "        loss = super()._one_batch(inputs, targets)\n",
    "        loss.backward()\n",
    "        self.scheduler.step()\n",
    "        self.optim.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x: torch.tensor, mean: float=0., std: float=1.) -> torch.tensor:\n",
    "    \"Truncated normal initialization.\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProdBias(nn.Module):\n",
    "    \"\"\"Same as DotProdBias, but scale the output to be within y_range.\"\"\"\n",
    "    def __init__(self, n_user: int, n_item: int, embedding_dim: int, y_range: typing.Tuple[int, int], trunc_normal: bool=False):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_embeddings=n_user, embedding_dim=embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_embeddings=n_item, embedding_dim=embedding_dim)\n",
    "        self.user_bias = nn.Embedding(num_embeddings=n_user, embedding_dim=1)\n",
    "        self.item_bias = nn.Embedding(num_embeddings=n_item, embedding_dim=1)\n",
    "        if trunc_normal:\n",
    "            # Based on\n",
    "            # https://github.com/fastai/fastai1/blob/6a5102ef7bdefa9058d0481ab311f48b21cbc6fc/fastai/layers.py#L285\n",
    "            for e in (self.user_emb, self.item_emb, self.user_bias, self.item_bias):\n",
    "                with torch.no_grad(): trunc_normal_(e.weight, std=0.01)\n",
    "        self.y_min, self.y_max = y_range\n",
    "    \n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # Convert from 1-based to 0-based index.\n",
    "        users, items = users - 1, items - 1\n",
    "        dot_prods = (self.user_emb(users) * self.item_emb(items)).sum(dim=1)\n",
    "        biased = dot_prods + self.user_bias(users) + self.item_bias(items)\n",
    "        return self.y_min + (self.y_max - self.y_min) * nn.functional.sigmoid(biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      7.755 |     6.754 | 4s\n    1 |      5.117 |     5.289 | 3s\n    2 |      3.834 |     4.599 | 3s\n    3 |      2.908 |     3.990 | 3s\n    4 |      2.060 |     3.375 | 3s\n    5 |      1.512 |     2.918 | 3s\n    6 |      1.257 |     2.601 | 3s\n    7 |      1.138 |     2.368 | 3s\n    8 |      1.079 |     2.189 | 3s\n    9 |      1.045 |     2.043 | 3s\n"
    }
   ],
   "source": [
    "model_scaled_dot_prod_bias = ScaledDotProdBias(n_user, n_item, 40, (-0.5, 5.5)).to(dev)\n",
    "fitter_scaled_dot_prod_bias = Fitter(model_scaled_dot_prod_bias, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias.parameters(), lr=5e-3, betas=(0.9, 0.99)))\n",
    "fitter_scaled_dot_prod_bias.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled_dot_prod_bias_path = \"mdoels/model_scaled_dot_prod_bias.pth\"\n",
    "torch.save(model_scaled_dot_prod_bias.state_dict(), model_scaled_dot_prod_bias_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      9.008 |     8.912 |   2s |\n    1 |      8.019 |     7.623 |   3s |\n    2 |      5.932 |     5.857 |   3s |\n    3 |      4.281 |     4.870 |   3s |\n    4 |      3.307 |     4.285 |   3s |\n    5 |      2.513 |     3.843 |   3s |\n    6 |      1.925 |     3.539 |   4s |\n    7 |      1.580 |     3.375 |   3s |\n    8 |      1.414 |     3.313 |   3s |\n    9 |      1.355 |     3.304 |   3s |\n"
    }
   ],
   "source": [
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (-0.5, 5.5)).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=5e-3, betas=(0.9, 0.99)))\n",
    "fitter_one_cycle.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      6.203 |     6.216 |   3s |\n    1 |      5.811 |     5.814 |   3s |\n    2 |      4.914 |     5.001 |   3s |\n    3 |      3.687 |     3.921 |   3s |\n    4 |      2.425 |     2.863 |   3s |\n    5 |      1.561 |     2.172 |   3s |\n    6 |      1.259 |     1.759 |   3s |\n    7 |      1.114 |     1.512 |   3s |\n    8 |      1.028 |     1.323 |   3s |\n    9 |      0.974 |     1.204 |   3s |\n   10 |      0.937 |     1.125 |   3s |\n   11 |      0.910 |     1.077 |   3s |\n   12 |      0.890 |     1.044 |   3s |\n   13 |      0.874 |     1.018 |   3s |\n   14 |      0.862 |     1.003 |   3s |\n   15 |      0.851 |     0.994 |   3s |\n   16 |      0.843 |     0.988 |   3s |\n   17 |      0.837 |     0.987 |   3s |\n   18 |      0.832 |     0.986 |   3s |\n   19 |      0.830 |     0.986 |   3s |\n"
    }
   ],
   "source": [
    "# Let's tweak y-range to start at 0.5\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5)).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99)))\n",
    "fitter_one_cycle.fit(num_epochs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      5.271 |     3.430 |   2s |\n    1 |      1.727 |     1.257 |   3s |\n    2 |      1.129 |     1.227 |   3s |\n    3 |      1.124 |     1.225 |   3s |\n    4 |      1.126 |     1.232 |   3s |\n    5 |      1.127 |     1.227 |   3s |\n    6 |      1.127 |     1.231 |   3s |\n    7 |      1.127 |     1.228 |   3s |\n    8 |      1.127 |     1.230 |   4s |\n    9 |      1.127 |     1.228 |   3s |\n   10 |      1.125 |     1.233 |   3s |\n   11 |      1.125 |     1.227 |   3s |\n   12 |      1.124 |     1.226 |   3s |\n   13 |      1.123 |     1.228 |   3s |\n   14 |      1.122 |     1.226 |   3s |\n   15 |      1.122 |     1.226 |   3s |\n   16 |      1.121 |     1.226 |   3s |\n   17 |      1.118 |     1.228 |   3s |\n   18 |      1.117 |     1.228 |   3s |\n   19 |      1.116 |     1.228 |   2s |\n"
    }
   ],
   "source": [
    "# Let's try weight decay\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5)).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99), weight_decay=1e-2))\n",
    "fitter_one_cycle.fit(num_epochs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      1.317 |     1.161 |   3s |\n    1 |      1.009 |     0.997 |   2s |\n    2 |      0.900 |     0.944 |   3s |\n    3 |      0.878 |     0.944 |   3s |\n    4 |      0.885 |     0.955 |   3s |\n    5 |      0.894 |     0.970 |   3s |\n    6 |      0.895 |     0.971 |   3s |\n    7 |      0.896 |     0.972 |   3s |\n    8 |      0.891 |     0.965 |   3s |\n    9 |      0.887 |     0.962 |   4s |\n   10 |      0.879 |     0.959 |   3s |\n   11 |      0.873 |     0.954 |   3s |\n   12 |      0.865 |     0.946 |   3s |\n   13 |      0.858 |     0.942 |   3s |\n   14 |      0.851 |     0.937 |   3s |\n   15 |      0.845 |     0.935 |   3s |\n   16 |      0.839 |     0.935 |   3s |\n   17 |      0.834 |     0.934 |   3s |\n   18 |      0.831 |     0.934 |   3s |\n   19 |      0.829 |     0.934 |   3s |\n"
    }
   ],
   "source": [
    "# Let's try trunc_normal initialization\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99)))\n",
    "fitter_one_cycle.fit(num_epochs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      1.392 |     1.336 |   3s |\n    1 |      1.164 |     1.230 |   3s |\n    2 |      1.122 |     1.226 |   3s |\n    3 |      1.125 |     1.227 |   3s |\n    4 |      1.126 |     1.230 |   3s |\n    5 |      1.127 |     1.226 |   3s |\n    6 |      1.127 |     1.235 |   3s |\n    7 |      1.128 |     1.229 |   3s |\n    8 |      1.127 |     1.227 |   3s |\n    9 |      1.126 |     1.230 |   3s |\n   10 |      1.125 |     1.230 |   3s |\n   11 |      1.125 |     1.233 |   3s |\n   12 |      1.125 |     1.222 |   3s |\n   13 |      1.124 |     1.228 |   3s |\n   14 |      1.122 |     1.223 |   3s |\n   15 |      1.122 |     1.227 |   3s |\n   16 |      1.121 |     1.227 |   3s |\n   17 |      1.119 |     1.227 |   3s |\n   18 |      1.117 |     1.228 |   3s |\n   19 |      1.115 |     1.228 |   3s |\n"
    }
   ],
   "source": [
    "# Wow, that really sped things up! Seems we're over-fitting after 5 epochs now. Let's try weight_decay again.\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(), torch.optim.Adam(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99), weight_decay=1e-2))\n",
    "fitter_one_cycle.fit(num_epochs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\n    0 |      1.264 |     1.072 |   3s |\n    1 |      0.935 |     0.953 |   4s |\n    2 |      0.896 |     0.958 |   4s |\n    3 |      0.893 |     0.959 |   4s |\n    4 |      0.886 |     0.944 |   6s |\n    5 |      0.876 |     0.938 |   5s |\n    6 |      0.863 |     0.929 |   5s |\n    7 |      0.850 |     0.924 |   4s |\n    8 |      0.839 |     0.923 |   4s |\n    9 |      0.831 |     0.923 |   4s |\n"
    }
   ],
   "source": [
    "# Hmm, under-fitting now :-(\n",
    "# Let's try using fastai's \"true_wd\" algorithm.\n",
    "# I think that's been added to torch as AdamW, so let's use that instead of Adam.\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(\n",
    "    model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(),\n",
    "    torch.optim.AdamW(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99)))\n",
    "fitter_one_cycle.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ScaledDotProdBias(\n  (user_emb): Embedding(943, 40)\n  (item_emb): Embedding(1682, 40)\n  (user_bias): Embedding(943, 1)\n  (item_bias): Embedding(1682, 1)\n)\nepoch | train_loss | test_loss | time\n    0 |      1.268 |     1.083 |   3s |\n    1 |      0.956 |     0.979 |   3s |\n    2 |      0.920 |     0.974 |   3s |\n    3 |      0.919 |     0.969 |   3s |\n    4 |      0.913 |     0.965 |   3s |\n    5 |      0.906 |     0.959 |   3s |\n    6 |      0.897 |     0.950 |   3s |\n    7 |      0.886 |     0.947 |   3s |\n    8 |      0.877 |     0.945 |   3s |\n    9 |      0.869 |     0.945 |   3s |\n"
    }
   ],
   "source": [
    "# try increasing weight_decay to what fastai uses.\n",
    "model_scaled_dot_prod_bias_one_cycle = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True).to(dev)\n",
    "print(model_scaled_dot_prod_bias_one_cycle)\n",
    "fitter_one_cycle = FitterOneCycle(\n",
    "    model_scaled_dot_prod_bias_one_cycle, nn.MSELoss(),\n",
    "    torch.optim.AdamW(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99),\n",
    "    weight_decay=1e-1))\n",
    "fitter_one_cycle.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch | train_loss | test_loss | time\ntensor([[ 488,  136],\n        [ 843,  582],\n        [  75,  235],\n        [ 275,  496],\n        [ 157,  476],\n        [  90,   86],\n        [ 378,  313],\n        [ 585, 1021],\n        [ 770,  473],\n        [ 416,  174],\n        [  16,  160],\n        [ 326,  427],\n        [ 537,  515],\n        [ 537,  509],\n        [ 567,  657],\n        [ 474,  651],\n        [ 655,  607],\n        [ 270,   90],\n        [ 821,  427],\n        [ 268,  235],\n        [ 747,   30],\n        [ 294,  240],\n        [  13,  669],\n        [ 452,   66],\n        [ 682,  801],\n        [ 385, 1135],\n        [ 895,  885],\n        [ 605,  873],\n        [ 524,  402],\n        [ 365,  846],\n        [ 806,  588],\n        [ 525,  147],\n        [ 452,  430],\n        [ 436,   38],\n        [ 406,  274],\n        [ 311, 1041],\n        [ 573,   50],\n        [ 183,  375],\n        [ 197,  385],\n        [  42,  282],\n        [  85,  414],\n        [ 703,  300],\n        [  21,  262],\n        [  13,  358],\n        [ 181, 1002],\n        [ 593,  223],\n        [ 532,  722],\n        [ 617,  192],\n        [ 405,  453],\n        [  90,  421],\n        [ 254,  649],\n        [ 880,  394],\n        [ 405,  365],\n        [ 937,  258],\n        [ 271,  210],\n        [ 504,  773],\n        [ 904,  794],\n        [ 807, 1411],\n        [ 472,  230],\n        [ 487,  226],\n        [ 648,  585],\n        [ 234,  588],\n        [ 276,  316],\n        [ 795,  727]])\ntorch.Size([64, 2])\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d3e432ad61c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_fastai\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     torch.optim.AdamW(model_fastai.parameters(), lr=1e-2, betas=(0.9, 0.99), weight_decay=1e-1))\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfitter_one_cycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-6563806262fc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             steps_per_epoch=math.ceil(len(train_df) / batch_size))\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6563806262fc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6563806262fc>\u001b[0m in \u001b[0;36m_one_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's try using fastai's model with my fitter.\n",
    "model_fastai = fastai.collab.EmbeddingDotBias(40, n_user, n_item, (0.5, 5.5))\n",
    "fitter_one_cycle = FitterOneCycle(\n",
    "    model_fastai, nn.MSELoss(),\n",
    "    torch.optim.AdamW(model_fastai.parameters(), lr=1e-2, betas=(0.9, 0.99), weight_decay=1e-1))\n",
    "fitter_one_cycle.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's basically identical to the performance of my model, so the difference is not\n",
    "# in the model architecture or parameter initialization but in the implementation of\n",
    "# fastai's fit_one_cycle vs FitterOneCycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.AdamW(model_scaled_dot_prod_bias_one_cycle.parameters(), lr=1e-2, betas=(0.9, 0.99),\n",
    "    weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ##\n",
    " * look into OneCycleLR parameters vs fastai's implementation\n",
    "    * max_lr is the same (1e-2).\n",
    "    * fastai has wd=1e-1, I'm using the default in torch.optim.AdamW which is 1e-2.\n",
    "    * moms vs base_momentum / max_momentum is the same.\n",
    "    * div_factor is the same.\n",
    "    * pct_start is the same.\n",
    "    * Not entirely sure, but I think final_div vs final_div_factor is the same.\n",
    " * But something is likely different either in OneCycleLR and/or in AdamW, because I think those are the main differences.\n"
   ]
  }
 ]
}