{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601593316060",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering\n",
    "\n",
    "An attempt to match fastai's performance on [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/), but with pure pytorch, not using fastai's library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import fastai\n",
    "import fastai.collab\n",
    "import fastai.datasets\n",
    "import fastai.tabular.transform\n",
    "import math\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import typing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cpu\") # Seems to be much faster than GPU for this application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_path = fastai.datasets.download_data(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", ext=\"\")\n",
    "dest_dir = zip_path.parent\n",
    "data_dir = os.path.splitext(zip_path)[0]\n",
    "if not os.path.exists(data_dir):\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "n_item: 1682, n_user: 943\n"
    }
   ],
   "source": [
    "col_names = (\"user\", \"item\", \"rating\", \"timestamp\")\n",
    "train_df = pandas.read_csv(os.path.join(data_dir, \"ua.base\"), sep=\"\\t\", names=col_names)\n",
    "test_df = pandas.read_csv(os.path.join(data_dir, \"ua.test\"), sep=\"\\t\", names=col_names)\n",
    "concat_df = pandas.concat((train_df, test_df))\n",
    "n_item = concat_df[\"item\"].max()\n",
    "n_user = concat_df[\"user\"].max()\n",
    "print(f\"n_item: {n_item}, n_user: {n_user}\")\n",
    "\n",
    "class MovieLensDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pandas.DataFrame, device: torch.device):\n",
    "        # Indices into embeddings need to have dtype \"long\".\n",
    "        self.ids_tensor = torch.tensor(df[[\"user\", \"item\"]].to_numpy(), dtype=torch.long, device=device)\n",
    "        self.ratings_tensor = torch.tensor(df[[\"rating\"]].to_numpy(), dtype=torch.float, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids_tensor[idx], self.ratings_tensor[idx]\n",
    "        \n",
    "\n",
    "train_dataset = MovieLensDataset(train_df, dev)\n",
    "test_dataset = MovieLensDataset(test_df, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_inputs, test_labels = test_dataset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai benchmark\n",
    "fastai claims state of the art performance, so let's start with that and see how it does.\n",
    "This is based on <https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson4-collab.ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>user</th>\n      <th>item</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>122</td>\n      <td>511</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <td>568</td>\n      <td>656</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <td>638</td>\n      <td>211</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>255</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>482</td>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Hacked up copy of CollabDataBunch.from_df because I want to use test_df as the validation set.\n",
    "user_name   = concat_df.columns[0]\n",
    "item_name   = concat_df.columns[1]\n",
    "rating_name = concat_df.columns[2]\n",
    "cat_names = [user_name,item_name]\n",
    "num_train = len(train_df)\n",
    "src = (fastai.collab.CollabList.from_df(concat_df, cat_names=cat_names, procs=fastai.tabular.transform.Categorify)\n",
    "        .split_by_idxs(train_idx=numpy.arange(num_train), valid_idx=numpy.arange(num_train, num_train + len(test_df)))\n",
    "        .label_from_df(cols=rating_name))\n",
    "data_bunch = src.databunch(path=\".\", bs=batch_size, val_bs=batch_size, device=dev)\n",
    "assert len(data_bunch.dl(fastai.basic_data.DatasetType.Train).dl.dataset.x) == num_train\n",
    "data_bunch.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.930389</td>\n      <td>1.043625</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.881159</td>\n      <td>0.995233</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.870824</td>\n      <td>0.959754</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.841655</td>\n      <td>0.946414</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.787724</td>\n      <td>0.941334</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.717227</td>\n      <td>0.917258</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.632812</td>\n      <td>0.902190</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.522469</td>\n      <td>0.895359</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.416748</td>\n      <td>0.898977</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.333631</td>\n      <td>0.900505</td>\n      <td>00:08</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fastai_learn = fastai.collab.collab_learner(data_bunch, n_factors=40, y_range=[0,5.5], wd=1e-1)\n",
    "fastai_learn.fit_one_cycle(num_epochs, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fastai_pred = fastai_learn.get_preds(ds_type=fastai.data_block.DatasetType.Valid)\n",
    "torch.nn.functional.mse_loss(*fastai_pred).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    def __init__(self, model: nn.Module, loss_func: nn.Module, optim: torch.optim.Optimizer):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.optim = optim\n",
    "        self.losses = []\n",
    "\n",
    "    def fit(self, num_epochs: int):\n",
    "        print(\"epoch | train_loss | test_loss | time\")\n",
    "        for epoch in range(num_epochs):\n",
    "            start = time.time()\n",
    "            train_loss = torch.tensor([0.0], dtype=float, device=dev)\n",
    "            self.model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader, 0):\n",
    "                train_loss += self._one_batch(inputs, targets)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Convert from 1-based to 0-based index.\n",
    "                pred = self.model(test_inputs[:, 0] - 1, test_inputs[:, 1] - 1)\n",
    "                test_loss = self.loss_func(pred, test_labels).item()\n",
    "\n",
    "            num_batches = batch_idx + 1\n",
    "            self.losses.append((train_loss / num_batches, test_loss))\n",
    "            print(\"%5d |      %.3f |     %.3f |   %ds |\" % (\n",
    "                epoch,\n",
    "                self.losses[-1][0],\n",
    "                self.losses[-1][1],\n",
    "                int(time.time() - start)))\n",
    "\n",
    "    def _one_batch(self, inputs: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "        self.optim.zero_grad()\n",
    "        # Convert from 1-based to 0-based index.\n",
    "        users, items = inputs[:, 0] - 1, inputs[:, 1] - 1\n",
    "        pred = self.model(users, items)\n",
    "        loss = self.loss_func(pred, targets)\n",
    "        # user_bias_params_before = list(model.user_bias.parameters())[0].clone()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        # user_bias_params_after = list(model.user_bias.parameters())[0].clone()\n",
    "        return loss.detach()\n",
    "\n",
    "class FitterOneCycle(Fitter):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.scheduler = None\n",
    "    \n",
    "    def fit(self, num_epochs: int):\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optim, max_lr=self.optim.defaults[\"lr\"],\n",
    "            final_div_factor=25e4, # fastai uses 25e4\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=math.ceil(len(train_df) / batch_size))\n",
    "        super().fit(num_epochs)\n",
    "\n",
    "    def _one_batch(self, inputs: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "        loss = super()._one_batch(inputs, targets)\n",
    "        self.scheduler.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x: torch.tensor, mean: float=0., std: float=1.) -> torch.tensor:\n",
    "    \"Truncated normal initialization.\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProdBias(nn.Module):\n",
    "    \"\"\"Same as DotProdBias, but scale the output to be within y_range.\"\"\"\n",
    "    def __init__(self, n_user: int, n_item: int, embedding_dim: int, y_range: typing.Tuple[int, int], trunc_normal: bool=False, squeeze=True):\n",
    "        super().__init__()\n",
    "        self.squeeze = squeeze\n",
    "        self.user_emb = nn.Embedding(num_embeddings=n_user, embedding_dim=embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_embeddings=n_item, embedding_dim=embedding_dim)\n",
    "        self.user_bias = nn.Embedding(num_embeddings=n_user, embedding_dim=1)\n",
    "        self.item_bias = nn.Embedding(num_embeddings=n_item, embedding_dim=1)\n",
    "        if trunc_normal:\n",
    "            # Based on\n",
    "            # https://github.com/fastai/fastai1/blob/6a5102ef7bdefa9058d0481ab311f48b21cbc6fc/fastai/layers.py#L285\n",
    "            for e in (self.user_emb, self.item_emb, self.user_bias, self.item_bias):\n",
    "                with torch.no_grad(): trunc_normal_(e.weight, std=0.01)\n",
    "        self.y_min, self.y_max = y_range\n",
    "    \n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "        dot_prods = (self.user_emb(users) * self.item_emb(items)).sum(dim=1)\n",
    "        u_bias = self.user_bias(users)\n",
    "        i_bias = self.item_bias(items)\n",
    "        if self.squeeze:\n",
    "            u_bias, i_bias = u_bias.squeeze(dim=1), i_bias.squeeze(dim=1)\n",
    "        biased = dot_prods + u_bias + i_bias\n",
    "        res = self.y_min + (self.y_max - self.y_min) * nn.functional.sigmoid(biased)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "squeeze=True\nepoch | train_loss | test_loss | time\n    0 |      1.338 |     1.277 |   2s |\n    1 |      1.294 |     1.301 |   2s |\n    2 |      1.293 |     1.284 |   2s |\n    3 |      1.280 |     1.267 |   3s |\n    4 |      1.273 |     1.266 |   3s |\nsqueeze=False\nepoch | train_loss | test_loss | time\n    0 |      1.181 |     0.997 |   2s |\n    1 |      0.911 |     0.957 |   3s |\n    2 |      0.887 |     0.936 |   3s |\n    3 |      0.862 |     0.925 |   4s |\n    4 |      0.840 |     0.923 |   3s |\n"
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-2\n",
    "print(\"squeeze=True\")\n",
    "model_squeeze = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True, squeeze=True).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(\n",
    "    model_squeeze,\n",
    "    nn.MSELoss(),\n",
    "    torch.optim.AdamW(model_squeeze.parameters(), lr=lr, betas=(0.9, 0.99), weight_decay=wd))\n",
    "fitter_one_cycle.fit(num_epochs // 2)\n",
    "\n",
    "print(\"squeeze=False\")\n",
    "model_squeeze = ScaledDotProdBias(n_user, n_item, 40, (0.5, 5.5), trunc_normal=True, squeeze=False).to(dev)\n",
    "fitter_one_cycle = FitterOneCycle(\n",
    "    model_squeeze,\n",
    "    nn.MSELoss(),\n",
    "    torch.optim.AdamW(model_squeeze.parameters(), lr=lr, betas=(0.9, 0.99), weight_decay=wd))\n",
    "fitter_one_cycle.fit(num_epochs // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ##\n",
    " * look into OneCycleLR parameters vs fastai's implementation\n",
    "    * learning rates are the same\n",
    "    * fastai's opt.mom is the same as pytorch's betas[0]\n",
    "    * what is betas[1]?\n",
    " * But something is likely different either in OneCycleLR and/or in AdamW, because I think those are the main differences.\n"
   ]
  }
 ]
}